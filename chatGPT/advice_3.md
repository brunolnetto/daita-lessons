Becoming a data engineer involves a combination of technical skills, tools, and a growth mindset. Here's a lean study routine and a list of essential platforms, tools, books, and mindset to help you become a data engineer:

**Study Routine:**

1. **Start with the Basics:**
   - Begin by building a strong foundation in computer science and programming. Learn a programming language like Python or Java, and understand data structures and algorithms.

2. **Databases and SQL:**
   - Focus on relational databases and SQL (Structured Query Language). Learn how to design, query, and manage databases.

3. **Big Data and Distributed Systems:**
   - Gain an understanding of distributed computing and big data technologies. Learn about Hadoop, Spark, and other distributed data processing frameworks.

4. **Data Modeling:**
   - Study data modeling techniques, including Entity-Relationship Diagrams (ERD) and schema design.

5. **ETL (Extract, Transform, Load):**
   - Learn ETL processes and tools like Apache Nifi, Apache Kafka, or Talend for data integration.

6. **Data Warehousing:**
   - Understand data warehousing concepts and platforms like Amazon Redshift, Google BigQuery, or Snowflake.

7. **Version Control:**
   - Familiarize yourself with version control systems like Git to manage your code and configurations.

8. **Cloud Platforms:**
   - Explore cloud platforms such as AWS, Azure, or Google Cloud, as they are widely used in data engineering.

9. **Automation and Orchestration:**
   - Learn tools like Apache Airflow for workflow automation and orchestration.

10. **Monitoring and Logging:**
    - Understand how to monitor and log data pipelines and systems for troubleshooting and optimization.

**Essential Platforms and Tools:**

- **Programming Languages:** Python, Java, Scala (for Spark)
- **Database Systems:** MySQL, PostgreSQL, Apache Cassandra
- **Big Data Tools:** Apache Hadoop, Apache Spark, Apache Kafka
- **Data Warehouses:** Amazon Redshift, Google BigQuery, Snowflake
- **ETL Tools:** Apache NiFi, Talend, Apache Beam
- **Workflow Orchestration:** Apache Airflow
- **Version Control:** Git
- **Cloud Platforms:** AWS, Azure, Google Cloud

**Recommended Books:**

1. "Designing Data-Intensive Applications" by Martin Kleppmann
2. "Data Engineering Cookbook" by Andreas Kretz
3. "Hadoop: The Definitive Guide" by Tom White
4. "Apache Spark in Action" by Jean-Georges Perrin
5. "Data Warehousing in the Age of Big Data" by Krish Krishnan
6. "SQL Performance Explained" by Markus Winand

**Mindset and Tips:**

1. **Continuous Learning:** The field of data engineering is constantly evolving, so embrace a mindset of continuous learning. Stay up-to-date with the latest tools and technologies.

2. **Problem-Solving:** Data engineering often involves solving complex problems. Develop strong problem-solving skills and a willingness to tackle challenging issues.

3. **Collaboration:** Effective communication and collaboration with data scientists, analysts, and other team members are crucial.

4. **Projects:** Apply what you learn by working on real-world projects. Building a portfolio of projects will demonstrate your skills to potential employers.

5. **Networking:** Attend meetups, conferences, and online forums to network with other data engineers and professionals in the field.

6. **Certifications:** Consider pursuing relevant certifications from cloud providers (e.g., AWS Certified Data Analytics, Google Professional Data Engineer).

7. **GitHub Portfolio:** Showcase your code and projects on GitHub to demonstrate your abilities to prospective employers.

Remember that becoming a data engineer is a journey, and it's important to be patient and persistent in your pursuit of knowledge and skills. Start with the basics and gradually build your expertise in data engineering technologies and practices.