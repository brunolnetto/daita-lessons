Becoming a data engineer requires a combination of technical skills, knowledge of data processing tools, and the right mindset for continuous learning and problem-solving. Here's a lean study routine to help you become a data engineer:

**1. Platforms and Tools:**

   - **Apache Hadoop:** Start with Hadoop, as it's a fundamental technology in the data engineering world. Learn to set up Hadoop clusters and work with HDFS (Hadoop Distributed File System).
   
   - **Apache Spark:** Gain proficiency in Apache Spark for distributed data processing. Understand Spark's core concepts, RDDs (Resilient Distributed Datasets), DataFrames, and Spark SQL.

   - **Apache Kafka:** Learn about event streaming and how to use Apache Kafka for real-time data pipelines.

   - **Apache Airflow:** Familiarize yourself with Apache Airflow for workflow automation and scheduling data pipelines.

   - **SQL and Databases:** Master SQL, as it's essential for querying and managing data in relational databases. Learn about popular databases like PostgreSQL, MySQL, and NoSQL databases like MongoDB or Cassandra.

   - **ETL Tools:** Explore ETL (Extract, Transform, Load) tools like Apache Nifi, Talend, or Informatica for data integration and transformation.

   - **Data Warehousing:** Understand data warehousing concepts and platforms like Amazon Redshift, Google BigQuery, or Snowflake.

   - **Version Control:** Learn Git for managing code and configuration changes.

**2. Books:**

   - **"Hadoop: The Definitive Guide" by Tom White:** A comprehensive guide to Hadoop and its ecosystem.

   - **"Spark: The Definitive Guide" by Bill Chambers and Matei Zaharia:** A valuable resource for learning Apache Spark.

   - **"Data Engineering Cookbook" by Andreas Kretz:** Offers practical insights into data engineering tasks and best practices.

   - **"Streaming Systems" by Tyler Akidau, Slava Chernyak, and Benjamin Zeller:** Helpful for understanding stream processing and Apache Kafka.

   - **"Data Warehousing in the Age of Big Data" by Krish Krishnan:** Focuses on modern data warehousing concepts.

**3. Mindset:**

   - **Continuous Learning:** Data engineering is a rapidly evolving field. Stay updated with the latest tools and technologies by reading blogs, attending webinars, and participating in online courses.

   - **Problem-Solving:** Develop a problem-solving mindset. Data engineering often involves debugging and optimizing data pipelines.

   - **Collaboration:** Work closely with data scientists, analysts, and other teams to understand their data requirements and deliver robust solutions.

   - **Attention to Detail:** Be meticulous in data quality and pipeline reliability. Errors in data engineering can have significant consequences downstream.

   - **Documentation:** Document your work, including pipeline configurations, code, and data schemas, for better collaboration and troubleshooting.

**4. Online Resources:**

   - Online courses on platforms like Coursera, edX, Udacity, and Pluralsight offer in-depth data engineering courses.
   
   - Explore blogs, forums, and YouTube channels dedicated to data engineering topics.

   - Contribute to open-source data engineering projects on GitHub to gain practical experience.

**5. Hands-On Practice:**

   - Create personal projects to apply your knowledge. Build data pipelines, work with real datasets, and solve data-related challenges.

   - Consider working on internships or freelance projects to gain practical experience.

Remember that becoming a data engineer takes time and dedication. Focus on building a strong foundation in the key technologies and tools mentioned above, and gradually expand your expertise as you gain experience. Stay curious and be open to exploring new tools and approaches as the field evolves.