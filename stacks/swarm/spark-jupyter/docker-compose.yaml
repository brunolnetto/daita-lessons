version: "3.8"

services:
  # üëë Apache Spark Master
  spark-master:
    image: bitnami/spark:4.0.0
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_UI=0.0.0.0
      - SPARK_HADOOP_FS_DEFAULTFS=hdfs://hadoop-namenode:8020
    ports:
      - "8080:8080"
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: "4G"
        reservations:
          cpus: "1.0"
          memory: "2G"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      retries: 3
      start_period: 30s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.spark-master.rule=Host(`spark-master.localhost`)"
      - "traefik.http.services.spark-master.loadbalancer.server.port=8080"
    networks:
      - traefik_network

  # üî• Apache Spark Workers
  spark-worker:
    image: bitnami/spark:4.0.0
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HADOOP_FS_DEFAULTFS=hdfs://hadoop-namenode:8020
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: "1.0"
          memory: "2G"
        reservations:
          cpus: "0.5"
          memory: "1G"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      retries: 3
      start_period: 30s
    labels:
      - "traefik.enable=false"
    networks:
      - traefik_network

  # üèóÔ∏è Hadoop Namenode
  hadoop-namenode:
    image: bde2020/hadoop-namenode:latest
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - dfs.namenode.datanode.registration.ip-hostname-check=false
    volumes:
      - hadoop-namenode:/hadoop/dfs/name
    ports:
      - "8020:8020"
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "2G"
        reservations:
          cpus: "0.5"
          memory: "1G"
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      retries: 3
    labels:
      - "traefik.enable=false"
    networks:
      - traefik_network

  # üì¶ Hadoop Datanodes
  hadoop-datanode:
    image: bde2020/hadoop-datanode:latest
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - HDFS_NAMENODE_URI=hdfs://hadoop-namenode:8020
    depends_on:
      hadoop-namenode:
        condition: service_healthy
    volumes:
      - hadoop-datanode:/hadoop/dfs/data
      - ./core-site.xml:/etc/hadoop/core-site.xml
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "2G"
        reservations:
          cpus: "0.5"
          memory: "1G"
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      retries: 3
    labels:
      - "traefik.enable=false"
    networks:
      - traefik_network

  # üßë‚Äçüè´ Jupyterlab
  jupyterlab:
    image: jupyter/pyspark-notebook:latest
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      - HDFS_URI=hdfs://hadoop-namenode:8020
    ports:
      - "8888:8888"
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "2G"
        reservations:
          cpus: "0.5"
          memory: "1G"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888"]
      retries: 3
      start_period: 30s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.jupyterlab.rule=Host(`jupyterlab.localhost`)"
      - "traefik.http.services.jupyterlab.loadbalancer.server.port=8888"
    networks:
      - traefik_network

volumes:
  hadoop-namenode:
  hadoop-datanode:

networks:
  traefik_network:
    external: true