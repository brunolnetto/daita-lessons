services:
  # üëë Apache Spark Master
  spark-master:
    image: bitnami/spark:3.5.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_UI=0.0.0.0
      - SPARK_HADOOP_FS_DEFAULTFS=hdfs://hadoop-namenode:8020
    ports:
      - "8080:8080"
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: "4G"
        reservations:
          cpus: "1.0"
          memory: "2G"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      retries: 3
      start_period: 30s

  # üî• Apache Spark Workers
  spark-worker:
    image: bitnami/spark:3.5.5
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_HADOOP_FS_DEFAULTFS=hdfs://hadoop-namenode:8020
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: "1.0"
          memory: "2G"
        reservations:
          cpus: "0.5"
          memory: "1G"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      retries: 3
      start_period: 30s

  # üèóÔ∏è Hadoop Namenode
  hadoop-namenode:
    image: bde2020/hadoop-namenode:latest
    container_name: hadoop-namenode
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - dfs.namenode.datanode.registration.ip-hostname-check=false
    volumes:
      - hadoop-namenode:/hadoop/dfs/name
    ports:
      - "8020:8020"
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "2G"
        reservations:
          cpus: "0.5"
          memory: "1G"
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      retries: 3

  # üì¶ Hadoop Datanodes
  hadoop-datanode:
    image: bde2020/hadoop-datanode:latest
    container_name: hadoop-datanode
    environment:
      - CLUSTER_NAME=hadoop-cluster
      - HDFS_NAMENODE_URI=hdfs://hadoop-namenode:8020
    depends_on:
      hadoop-namenode:
        condition: service_healthy
    volumes:
      - hadoop-datanode:/hadoop/dfs/data
      - ./core-site.xml:/etc/hadoop/core-site.xml
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "2G"
        reservations:
          cpus: "0.5"
          memory: "1G"
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-report"]
      retries: 3

  # üßë‚Äçüè´ Jupyterlab
  jupyterlab:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyterlab
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      - HDFS_URI=hdfs://hadoop-namenode:8020  # Add HDFS URI
    ports:
      - "8888:8888"
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: "2G"
        reservations:
          cpus: "0.5"
          memory: "1G"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888"]
      retries: 3
      start_period: 30s

volumes:
  hadoop-namenode:
  hadoop-datanode:
